Tale descrizione deve contemplare almeno gli elementi riportati nelle seguenti
sottosezioni.

\subsection{Convolutional neural network}
\emph{Descrivere in questa sezione l’architettura della convolutional neural network
sviluppata nell’ambito del progetto. Se si utilizza un’architettura nota, si riportino gli elementi fondamentali della rete e le eventuali modifiche effettuate ma, soprattutto, si motivi la scelta.
Definire in ogni caso se la rete è stata progettata come regressore (output:
numero reale da 0 a 100 che rappresenta l’età, da approssimare all’intero più
vicino) o come classificatore (output: una delle 101 classi da 0 a 100) e fornire dettagli e motivazioni sulla funzione di costo scelta.}

L'architettura scelta è MobileNetV3.\todo{citazione del MIVIA?} Le reti MobileNet sono conosciute per essere progettate pensando all'efficienza computazionale e alla latenza delle predizioni, senza però sacrificare le prestazioni. MobileNetV3 migliora MobileNetV2 sia in termini di accuracy che di latenza, diminuendo il numero di pesi ed il numero di operazioni totali. Questo ci ha permesso di allenare la rete e di testarla in tempi ragionevoli rapportati al tempo disponibile, e quindi ha permesso una scelta migliore dei parametri.
La rete è stata progettata come un regressore. Abbiamo sostituito il layer di classificazione con un layer di dimensione unitaria con attivazione lineare (ReLU). Questo ci ha permesso di astrarci dalla rappresentazione dell'età, che non è per forza definita nell'intervallo \([0, 100]\), e l'utilizzo della ben nota in letteratura Mean Squared Error (MSE) loss function, che tiene anche conto della differenza tra la stima dell'età e la \emph{ground truth}.

\subsection{Procedura di addestramento}
\subsubsection{Dataset}
\label{subsubsec:dataset}

Il dataset utilizzato per allenare la nostra rete a compiere il task assegnato è il dataset chiamato \emph{VGG-Face2 Mivia Age}.

Quest'ultimo dataset contiene $3.31$ milioni di immagini di 9131 soggetti, con una media di $362.6$ immagini per ogni soggetto. Le immagini sono scaricate da Google Image Search e si diversificano molto per posa, età, illuminazione, etnia e professione del soggetto catturato. Ad ogni immagine è associata l'età del soggetto, tale valore è il risultato di un ensemble di $14$ modelli CNN.\\
Il dataset ci è stato fornito già diviso in training set, che include $8631$ identità, e test set, che include le rimanenti 500.

\todo{Non mi piace la seguente frase, aiuto} Le risorse che \textit{Google Colab} mette a disposizione sono risultate insufficienti per allenare la rete sull'intero training set. È risultato necessario ridurre la grandezza di quest'ultimo, a tale scopo è stata svolta un analisi del training set e sono stati scelte le identità su cui svolgere l'addestramento della rete. Tale analisi è stata svolta anche con il supporto delle annotazioni per VGGFace2, reperibili \href{https://github.com/MiviaLab/GenderRecognitionFramework/releases/tag/0}{qui}.

Con lo scopo di svolgere l'allenamento su un training set più bilanciato possibile, abbiamo svolto una prima analisi sul genere delle identità presenti.

\begin{figure}[H]

\begin{subfigure}{0.5\textwidth}
\def\svgscale{0.5}
\input{./Images/gender_ids.pdf_tex}
\caption{Identità divise per genere}
\label{sfig:Ids per gender}
\end{subfigure}
\begin{subfigure}{0.5\textwidth}
\def\svgscale{0.5}
\input{./Images/gender_images.pdf_tex}
\caption{Immagini divise per genere}
\label{sfig:Images per gender}
\end{subfigure}
\caption{Divisione per genere di identità e immagini}
\label{fig:gender_division}
\end{figure}

Come possiamo vedere in \ref{sfig:Ids per gender} e in \ref{sfig:Images per gender}, il training set risulta sbilanciato per quanto riguarda il genere dei soggetti.

L'analisi si è successivamente concentrata sulla \emph{media e deviazione standard} dell'età di ogni soggetto presente nel training set.

\begin{figure}[H]

\begin{subfigure}{0.5\textwidth}
\def\svgscale{0.42}
\input{./Images/n_ids_by_age_and_gender.pdf_tex}
\caption{Identità divise per genere ed età media}
\label{sfig:Ids per gender and mean age}
\end{subfigure}
\begin{subfigure}{0.5\textwidth}
\def\svgscale{0.42}
\input{./Images/n_images_by_age_and_gender.pdf_tex}
\caption{Immagini divise per genere ed età media}
\label{sfig:Images per gender and mean age}
\end{subfigure}
\caption{Divisione per genere di identità e immagini}
\label{fig:gender_age_division}
\end{figure}

Come si evince dai grafici in figura~\ref{sfig:Ids per gender and mean age} e in figura~\ref{sfig:Images per gender and mean age}, ci sono delle fasce d'età sovrarappresentate rispetto alle altre, in particolare le fasce d'età $[25,34]$ e $[35,44]$.

Con lo scopo di avere un training set quanto più bilanciato possibile, abbiamo scelto di escludere dal set gli uomini nelle fasce: $[25,34]$ e $[35,44]$; e le donne nella fascia $[25,34]$; la cui deviazione standard dell'età risulta inferiore a $5.5$. Il risultato di tale operazione è visibile in figura~\ref{fig:Ids per gender and mean age after the drop}.

\begin{figure}[H]
\centering
\def\svgscale{0.7}
\input{./Images/n_ids_by_age_and_gender_after_drop.pdf_tex}
\caption{Identità divise per genere ed età media dopo il taglio}
\label{fig:Ids per gender and mean age after the drop}
\end{figure}

Successivamente a tale operazione, al training set sono state sottratte ulteriori $500$ identità, scelte casualmente, che hanno formato il \emph{validation set}.

\subsubsection{Face detection}
\label{subsubsec:face_detection}

\emph{Descrivere il metodo utilizzato per effettuare il rilevamento del volto. Se si utilizza un approccio noto o i volti già estratti con framework esistenti, si specifichi questa informazione.}

Non è stato effettuato alcun rilevamento del volto. Nelle annotazioni del dataset VGGFAce2, già citate nella sezione~\ref{subsubsec:dataset}, sono riportate sia per il training set che per il test set, informazioni inerenti alle bounding boxes relative ai volti dei soggetti catturati. Tali informazioni definiscono quindi la \textbf{region of interest (\textbf{roi})} di ogni immagine.

\subsubsection{Face pre-processing} 

\emph{Descrivere e motivare tutte le tecniche di pre-processing applicate sulle immagini del volto.}

Si distinguono due fasi del pre-processing delle immagini, la prima fase viene effettuata a monte della data augmentation mentre la seconda a valle di quest'ultima.
Durante la prima fase viene ritagliato il volto del soggetto. Il ritaglio non viene effettuato però sulla roi (sezione~\ref{subsubsec:face_detection}), infatti la region of interest viene allargata di un fattore $0.3$, ovviamente non andando oltre i limiti dettati dalla risoluzione dell'immagine, tale operazione permette di ritagliare oltre al volto del soggetto l'intera testa.
Nella seconda fase ogni immagine viene prima ridimensionata alla risoluzione \todo{Attenzione alla risoluzione} $96 \times 96$ mantenendo le proporzioni, nel caso in cui l'immagine non fosse quadrata viene \todo{in che senso riempita?} riempita con delle bande nere. Successivamente il valore medio di ogni canale è sottratto da ogni pixel, ed infine viene convertita da BGR ad RGB.

\subsubsection{Data augmentation}
Descrivere e motivare tutte le policy di augmentation
implementate per estendere il dataset o per aumentarne la rappresentativit`a.

\subsection{Training from scratch o fine tuning}
Specificare se la rete viene addestrata
con inizializzazione random o partendo da pesi pre-addestrati, motivando la
scelta e fornendo dettagli sulla strategia di inizializzazione.

\subsubsection{Procedura di training}
Dettagliare e motivare almeno le seguenti scelte: numero di epoche di addestramento, tipo di ottimizzatore, learning rate scheduling (tecnica di riduzione, learning rate iniziale, fattore di riduzione). Fornire dettagli su eventuali elementi aggiuntivi: batch normalization, weight decay, early stopping etc. Per ognuna delle scelte, riportare i valori esatti dei parametri utilizzati, per rendere l’esperimento riproducibile. Motivare la scelta di tali valori.